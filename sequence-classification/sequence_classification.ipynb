{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RPd99oqfUi36",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 162.0
    },
    "outputId": "00907928-277b-4685-a612-265bda2d8ab6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.52463299332E12,
     "user_tz": -720.0,
     "elapsed": 42950.0,
     "user": {
      "displayName": "Edison",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "103264836563542052392"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading https://files.pythonhosted.org/packages/78/bc/de067ab2d700b91717dc5459d86a1877e2df31abfb90ab01a5a5a5ce30b4/tqdm-4.23.0-py2.py3-none-any.whl (42kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 3.2MB/s \n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.23.0\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# http://pytorch.org/\n",
    "from os import path\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "\n",
    "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n",
    "\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CbU_OgLCVN_0",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 234.0
    },
    "outputId": "00f1f957-8cc3-4b29-a224-22cee6e617a9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.52463307738E12,
     "user_tz": -720.0,
     "elapsed": 11622.0,
     "user": {
      "displayName": "Edison",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "103264836563542052392"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-04-25 05:11:07--  http://ai.stanford.edu/%7Eamaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘aclImdb_v1.tar.gz’\n",
      "\n",
      "aclImdb_v1.tar.gz   100%[===================>]  80.23M  75.2MB/s    in 1.1s    \n",
      "\n",
      "2018-04-25 05:11:08 (75.2 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download the Large IMDB Movie Review Dataset\n",
    "# http://ai.stanford.edu/%7Eamaas/data/sentiment/index.html\n",
    "! wget http://ai.stanford.edu/%7Eamaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "! tar -xzf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "mif0OHyAVcqM",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import os\n",
    "from collections import namedtuple\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "yvd2UgrY3lSU",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "# let's set some parameters\n",
    "train_path = \"aclImdb/train/\" \n",
    "test_path = \"aclImdb/test/\"\n",
    "\n",
    "batch_size = 100\n",
    "embedding_size = 300\n",
    "min_count = 2\n",
    "cuda = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckgNJjRgqW2k",
    "colab_type": "text"
   },
   "source": [
    "## Load the dataset\n",
    "- 25000 train and test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "_jphx0DE3iOO",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "Sentence = namedtuple('Sentence', ['index', 'string', 'label'])\n",
    "\n",
    "def read_imdb_movie_dataset(dataset_path):\n",
    "\n",
    "    indices = []\n",
    "    text = []\n",
    "    rating = []\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for filename in os.listdir(os.path.join(dataset_path, \"pos\")):\n",
    "        file_path = os.path.join(dataset_path, \"pos\", filename)\n",
    "        data = open(file_path, 'r', encoding=\"ISO-8859-1\").read()\n",
    "        indices.append(i)\n",
    "        text.append(data)\n",
    "        rating.append(1)\n",
    "        i = i + 1\n",
    "\n",
    "    for filename in os.listdir(os.path.join(dataset_path, \"neg\")):\n",
    "        file_path = os.path.join(dataset_path, \"neg\", filename)\n",
    "        data = open(file_path, 'r', encoding=\"ISO-8859-1\").read()\n",
    "        indices.append(i)\n",
    "        text.append(data)\n",
    "        rating.append(0)\n",
    "        i = i + 1\n",
    "\n",
    "    sentences = [ Sentence(index, text, rating)\n",
    "                  for index, text, rating in zip(indices, text, rating)]\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CNpRcBNj3pxA",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 53.0
    },
    "outputId": "9a783614-6bcf-4f99-bb30-ffb66daa257f",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.524633116082E12,
     "user_tz": -720.0,
     "elapsed": 4632.0,
     "user": {
      "displayName": "Edison",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "103264836563542052392"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "train_sentences = read_imdb_movie_dataset(train_path)\n",
    "test_sentences = read_imdb_movie_dataset(test_path)\n",
    "\n",
    "random.shuffle(train_sentences)\n",
    "random.shuffle(test_sentences)\n",
    "\n",
    "print(len(train_sentences))\n",
    "print(len(test_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWv1Lme8qhQW",
    "colab_type": "text"
   },
   "source": [
    "## Mapping our words to unique identifiers: the Vocabulary object\n",
    "- We will create an object to manage a mapping between words (or more generally tokens) and unique indices. \n",
    "- There are a few special symbols that we will be adding to handle special cases.\n",
    "  - The first key special case is the `UNK` token, wich will represent all tokens that we do not have in our vocabulary. This is needed as we will build our vocabulary only using the training examples, and during validation or testing (or if we deploy our model in production) we may encounter new words that also need to be represented somehow.\n",
    "  - The `PAD` token, which we will use to create even-sized batches of sentences of different length (more on this below). \n",
    "  - The beginning-of-sentence or `BOS` token, which we may use to denote the beginning of a sentence in some special cases\n",
    "  - The end-of-sentence or `EOS` token, which as in the previous case is useful for certain tasks.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "-iLmpHZEVJHY",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "# Define the string of special tokens we will need \n",
    "UNK = '<UNK>'\n",
    "PAD = '<PAD>'\n",
    "BOS = '<BOS>'\n",
    "EOS = '<EOS>'\n",
    "\n",
    "\n",
    "class VocabItem:\n",
    "\n",
    "    def __init__(self, string, hash=None):\n",
    "        \"\"\"\n",
    "        Our token object, representing a term in our vocabulary.\n",
    "        \"\"\"\n",
    "        self.string = string\n",
    "        self.count = 0\n",
    "        self.hash = hash\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        For pretty-printing of our object\n",
    "        \"\"\"\n",
    "        return 'VocabItem({})'.format(self.string)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        For pretty-printing of our object\n",
    "        \"\"\"\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "def tokenizer(x):\n",
    "    return x.split()\n",
    "\n",
    "def token_function(x):\n",
    "    return x.lower()\n",
    "\n",
    "class Vocab:\n",
    "\n",
    "    def __init__(self, sentences, tokenizer=tokenizer,\n",
    "                 token_function=token_function, min_count=0,\n",
    "                 add_padding=False, add_bos=False, add_eos=False, unk=None):\n",
    "        \"\"\"\n",
    "        :param sentences: A list of strings.\n",
    "\n",
    "        :param tokenizer: A function to tokenize strings into tokens.\n",
    "\n",
    "        :param token_function: A function to process every token string,\n",
    "                               useful for normalizing case, and handling\n",
    "                               numbers, dates and so.  \n",
    "\n",
    "        :param add_padding: if we should add the special `PAD` token.\n",
    "\n",
    "        :param add_bos: If we should add the special `BOS` token.\n",
    "\n",
    "        :param add_eos: If we should add the special `EOS` token.\n",
    "\n",
    "        :param unk: A string with the unknown token, in case our \n",
    "                    sentences have already been processed for this,\n",
    "                    or `None` to use our default `UNK` token. \n",
    "\n",
    "        :param min_count: The minimum frequency count threshold for a token\n",
    "                          to be added to our mapping. Only useful if \n",
    "                          the unk parameter is None.\n",
    "\n",
    "        \"\"\"\n",
    "        vocab_items = []\n",
    "        vocab_hash = {}\n",
    "        word_count = 0\n",
    "\n",
    "        self.token_function = token_function\n",
    "        self.tokenizer = tokenizer\n",
    "        self.special_tokens = []\n",
    "\n",
    "        self.UNK = None\n",
    "        self.PAD = None\n",
    "        self.BOS = None\n",
    "        self.EOS = None\n",
    "\n",
    "        index2token = []\n",
    "        token2index = {}\n",
    "\n",
    "        # we tokenize or sentences, process our tokens\n",
    "        # and add them to a list of VocabItem objects\n",
    "        for sentence in sentences:\n",
    "            for token in tokenizer(sentence.string):\n",
    "                real_token = token_function(token)\n",
    "                if real_token not in vocab_hash:\n",
    "                    vocab_hash[real_token] = len(vocab_items)\n",
    "                    vocab_items.append(VocabItem(real_token))\n",
    "\n",
    "                vocab_items[vocab_hash[real_token]].count += 1\n",
    "                word_count += 1\n",
    "\n",
    "        tmp = []\n",
    "\n",
    "        # we add/handle the special `UNK` token\n",
    "        # and set it to have index 0 in our mapping \n",
    "        if unk:\n",
    "            self.UNK = VocabItem(unk, hash=0)\n",
    "            self.UNK.count = vocab_items[vocab_hash[unk]].count\n",
    "            index2token.append(self.UNK)\n",
    "            self.special_tokens.append(self.UNK)\n",
    "\n",
    "            for token in vocab_items:\n",
    "                if token.string != unk:\n",
    "                    tmp.append(token)\n",
    "\n",
    "        else:\n",
    "            self.UNK = VocabItem(UNK, hash=0)\n",
    "            index2token.append(self.UNK)\n",
    "            self.special_tokens.append(self.UNK)\n",
    "\n",
    "            for token in vocab_items:\n",
    "                if token.count <= min_count:\n",
    "                    self.UNK.count += token.count\n",
    "                else:\n",
    "                    tmp.append(token)\n",
    "\n",
    "        # we sort our vocab. items by frequency\n",
    "        # so for the same corpus, the indices of our words\n",
    "        # are always the same\n",
    "        tmp.sort(key=lambda token: token.count, reverse=True)\n",
    "\n",
    "        # we always add our additional special tokens\n",
    "        # at the end of our mapping\n",
    "        if add_bos:\n",
    "            self.BOS = VocabItem(BOS)\n",
    "            tmp.append(self.BOS)\n",
    "            self.special_tokens.append(self.BOS)\n",
    "\n",
    "        if add_eos:\n",
    "            self.EOS = VocabItem(EOS)\n",
    "            tmp.append(self.EOS)\n",
    "            self.special_tokens.append(self.EOS)\n",
    "\n",
    "        if add_padding:\n",
    "            self.PAD = VocabItem(PAD)\n",
    "            tmp.append(self.PAD)\n",
    "            self.special_tokens.append(self.PAD)\n",
    "\n",
    "        index2token += tmp\n",
    "\n",
    "        # we update the vocab_hash for each \n",
    "        # VocabItem object in our list \n",
    "        # based on their frequency \n",
    "        for i, token in enumerate(index2token):\n",
    "            token2index[token.string] = i\n",
    "            token.hash = i\n",
    "\n",
    "        self.index2token = index2token\n",
    "        self.token2index = token2index\n",
    "\n",
    "        print('Unknown vocab size:', self.UNK.count)\n",
    "        print('Vocab size: %d' % len(self))\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.index2token[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index2token)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.index2token)\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        return key in self.token2index\n",
    "\n",
    "    def string2indices(self, string, add_bos=False, add_eos=False):\n",
    "        \"\"\"\n",
    "        Returns a list of mapping indices by processing the given string\n",
    "        with our `tokenizer` and `token_function`, and defaulting to our\n",
    "        special `UNK` token whenever we found an unseen term.\n",
    "        \n",
    "        :param string: A sentence string we wish to map into our vocabulary.\n",
    "        \n",
    "        :param add_bos: If we should add the `BOS` at the beginning.\n",
    "        \n",
    "        :param add_eos: If we should add the `EOS` at the end.\n",
    "        \n",
    "        :return: A list of ints, with the indices of each token in the\n",
    "                 given string.\n",
    "        \n",
    "        \"\"\"\n",
    "        string_seq = []\n",
    "        if add_bos:\n",
    "            string_seq.append(self.BOS.hash)\n",
    "        for item in self.tokenizer(string):\n",
    "            processed_token = self.token_function(item)\n",
    "            string_seq.append(self.token2index.get(processed_token, self.UNK.hash))\n",
    "        if add_eos:\n",
    "            string_seq.append(self.EOS.hash)\n",
    "        return string_seq\n",
    "\n",
    "\n",
    "    def indices2tokens(self, indices, ignore_ids=()):\n",
    "        \"\"\"\n",
    "        Retuns a list of strings by mapping back every index to our\n",
    "        vocabulary.\n",
    "        \n",
    "        :param indices: A list of ints. \n",
    "        \n",
    "        :param ignore_ids: An itereable with indices to ignore, meaning\n",
    "                           that we will not look for them in our mapping.\n",
    "        \n",
    "        :return: A list of strings.\n",
    "        \n",
    "        Will raise an Exception whenever we pass an index that we\n",
    "        do not have in our mapping, except when provided with `ignore_ids`.\n",
    "        \n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "\n",
    "        for idx in indices:\n",
    "            if idx in ignore_ids:\n",
    "                continue\n",
    "            tokens.append(self.index2token[idx])\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8wDnRfy1fp-o",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 53.0
    },
    "outputId": "56172299-5ff8-46d0-9143-f5f45da3c4d1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.524633304022E12,
     "user_tz": -720.0,
     "elapsed": 6690.0,
     "user": {
      "displayName": "Edison",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "103264836563542052392"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown vocab size: 213668\n",
      "Vocab size: 69448\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(train_sentences,\n",
    "              min_count=min_count,\n",
    "              add_padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CDcHKXM3DT4K",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 35.0
    },
    "outputId": "f545e262-310a-4afe-8046-c976686bb34d",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.524633406574E12,
     "user_tz": -720.0,
     "elapsed": 1000.0,
     "user": {
      "displayName": "Edison",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "103264836563542052392"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 19, 13, 96]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.string2indices('the movie was bad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSGDb0MH9A_G",
    "colab_type": "text"
   },
   "source": [
    "## Representing words using dense vectors: Word Embeddings\n",
    "- One of the major breakthroughs in NLP with deep models came after the conception of word embeddings, which changed the way in which we represent each word in our machine learning models.\n",
    "- We start by simply assigning an initially random vector to each word in our vocabulary.These vectors are stacked together into a big matrix, usually referred to as the *embedding* matrix. After we have built our vocabulary, all we have to do is to create a big tensor of shape (`vocab_size`, `embedding_size`).\n",
    "- In theory, whenever we need to obtain the vector for a given word, we could build a one-hot vector of our word and multply this vector by our *embedding* matrix. All but one value in this one-hot vector are zeroes, the result of this product will correspond exactly to the vector that represents our word.\n",
    "- Our *embeddings* will be treated as parameters of our models and are trained with it. This is possible because the *embedding* mechanism as has a well-defined derivative, so we are  allowed to use backpropagation to train these vectors.\n",
    "- Note that in practice, however, the one-hot-based behavior can be achieved by simply selecting row vectors from our *embedding* matrix, given our indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "wF9Ef_2n82ny",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "embeddings = nn.Embedding(len(vocab.index2token),\n",
    "                           embedding_size,\n",
    "                           padding_idx=vocab.PAD.hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4MRXrQ66J230",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 35.0
    },
    "outputId": "7ea88533-02c3-4f9c-a57c-fada63c1a1bd",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.52463342787E12,
     "user_tz": -720.0,
     "elapsed": 1020.0,
     "user": {
      "displayName": "Edison",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "103264836563542052392"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69448, 300])\n"
     ]
    }
   ],
   "source": [
    "print(embeddings.weight.data.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kG5YLzgxFGE",
    "colab_type": "text"
   },
   "source": [
    "## The BatchIterator object\n",
    "- We will create an object to help us transform our text data into tensors with information that can be fed into our neural network. This object will do all the heavy-lifting, turning our string examples into batches of sequences of word indices that PyTorch can handle.\n",
    "### The padding function\n",
    "- Let's suppose we have these two sentences to build a batch:\n",
    "  - the dog barks $\\rightarrow [1, 2 ,3]$\n",
    "  - the cat likes to sleep $\\rightarrow [1, 4, 5, 6, 7]$\n",
    "  \n",
    "  In order to put these two examples in a batch Tensor, we will need to *pad* the shortest sentence to have the same length of the longest one. \n",
    "  - the dog barks $\\rightarrow [1, 2 ,3, 0 , 0]$\n",
    "  - the cat likes to sleep $\\rightarrow [1, 4, 5, 6, 7]$\n",
    "  \n",
    "  Finally, our batch Tensor will look like this: \n",
    "  - $\\begin{bmatrix}1 & 2 & 3 & 0 & 0 \\\\ 1 & 4 & 5 & 6 & 7\\end{bmatrix}$\n",
    "  \n",
    "  where its first dimension represents the size of the batch, and its second dimension has the length of the longest sentence in our batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "J8eXpGbS8BJW",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "def pad_list(raw_input_list, dim0_pad=None, dim1_pad=None,\n",
    "             align_right=False, pad_value=0):\n",
    "    \"\"\"\n",
    "    Receive a list of lists and return a padded 2d torch tensor,\n",
    "    a list of lengths and a padded mask\n",
    "    input_list: a list of lists. len(input_list) = M, and N is the max\n",
    "    length of any of the lists contained in input_list.\n",
    "        e.g.: [[2,45,3,23,54], [12,4,2,2], [4], [45, 12]]\n",
    "    \n",
    "    Return a torch tensor of dimension (M, N) corresponding to the padded\n",
    "    sequence, a list of the original lengths, and a mask\n",
    "    \n",
    "    Returns:\n",
    "         out: a torch tensor of dimension (M, N)\n",
    "         lengths: a list of ints containing the lengths of each input_list\n",
    "                  element\n",
    "\n",
    "     \"\"\"\n",
    "    input_list = [torch.LongTensor(sublist) for sublist in raw_input_list]\n",
    "    \n",
    "    if not dim0_pad:\n",
    "        dim0_pad = len(input_list)\n",
    "\n",
    "    if not dim1_pad:\n",
    "        dim1_pad = max(x.size(0) for x in input_list)\n",
    "\n",
    "    out = input_list[0].new(dim0_pad, dim1_pad).fill_(pad_value)\n",
    "\n",
    "    lengths = []\n",
    "    for i in range(len(input_list)):\n",
    "        data_length = input_list[i].size(0)\n",
    "        data_length = data_length if data_length < dim1_pad else dim1_pad\n",
    "        lengths.append(data_length)\n",
    "        offset = dim1_pad - data_length if align_right else 0\n",
    "        out[i].narrow(0, offset, data_length).copy_(input_list[i])\n",
    "\n",
    "    return out, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "s1G3uXgc4YkA",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "class BatchIterator(object):\n",
    "\n",
    "    def __init__(self, sentences, vocab, batch_size,\n",
    "                 shuffle=False, cuda=False, ids=None):\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.pad_id = self.vocab.PAD.hash\n",
    "\n",
    "        self.id_examples = []\n",
    "        self.examples = []\n",
    "        self.y_examples = []\n",
    "\n",
    "        # we create a list with our examples as Tensor objects\n",
    "        # we keep a list with the ids for each example, which\n",
    "        # is special for ().\n",
    "        for i, sentence in enumerate(sentences):\n",
    "\n",
    "            example = vocab.string2indices(sentence.string)\n",
    "\n",
    "            self.examples.append(torch.LongTensor(example))\n",
    "\n",
    "            if sentence.index is not None:\n",
    "                self.id_examples.append(int(sentence.index))\n",
    "            else:\n",
    "                self.id_examples.append(i)\n",
    "\n",
    "            y_example = int(sentence.label)\n",
    "            self.y_examples.append(torch.LongTensor([y_example]))\n",
    "\n",
    "        assert len(self.examples) == len(self.y_examples)\n",
    "\n",
    "\n",
    "        self.cuda = self.is_cuda = cuda\n",
    "\n",
    "        self.num_batches = (len(self.examples) + batch_size - 1) // batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Overload the `len()` Python syntax.\n",
    "        \"\"\"\n",
    "        return self.num_batches\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        By implementing this function, we allow our BatchIterator\n",
    "        to be an iterated over. Every time we reach the end of\n",
    "        our examples, if the `shuffle` parameter was provided\n",
    "        we shuffle our examples. \n",
    "        To tell Python that we have reached the end of our set\n",
    "        to iterate on, we must raise the IndexError, which the Python\n",
    "        interpreter takes to stop the iteration process.\n",
    "\n",
    "        \"\"\"\n",
    "        if index >= self.num_batches:\n",
    "\n",
    "            if self.shuffle:\n",
    "                c = list(zip(self.id_examples,\n",
    "                             self.examples,\n",
    "                             self.y_examples))\n",
    "\n",
    "                random.shuffle(c)\n",
    "\n",
    "                (self.id_examples, self.examples, self.y_examples) = zip(*c)\n",
    "\n",
    "            raise IndexError(\"Index is greater \"\n",
    "                             \"than the number of batches\")\n",
    "\n",
    "        # first we obtain the batch slice indices\n",
    "        start = index * self.batch_size\n",
    "        end = (index + 1) * self.batch_size\n",
    "        \n",
    "        # we get id, x and y items corresonding to the current iteration\n",
    "        id_slice = self.id_examples[start:end]\n",
    "        x_slice = self.examples[start:end]\n",
    "        y_slice = self.y_examples[start:end]\n",
    "\n",
    "        # we need to pad our examples (explanation below)\n",
    "        padded_x_slice, x_slice_lengths = pad_list(x_slice,\n",
    "                                                   pad_value=self.pad_id)\n",
    "        y_slice = torch.cat(y_slice, 0)\n",
    "\n",
    "        padded_x_slice = Variable(padded_x_slice)\n",
    "        y_slice = Variable(y_slice)\n",
    "        \n",
    "        # we move our Tensors to the GPU if needed\n",
    "        if self.cuda:\n",
    "            padded_x_slice = padded_x_slice.cuda()\n",
    "            y_slice = y_slice.cuda()\n",
    "\n",
    "        return id_slice, padded_x_slice, x_slice_lengths, y_slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1V-6QNUDnFA",
    "colab_type": "text"
   },
   "source": [
    "Let's instance our `batch_iterator` objects for the training and test examples, and inspect a single batch of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "OQTU7ZG691DG",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "train_batches = BatchIterator(train_sentences,\n",
    "                              vocab,\n",
    "                              batch_size,\n",
    "                              cuda=cuda)\n",
    "\n",
    "\n",
    "test_batches = BatchIterator(test_sentences,\n",
    "                             vocab,\n",
    "                             batch_size,\n",
    "                             cuda=cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "H5aoUJjfDx92",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 91.0
    },
    "outputId": "94734025-6b8e-46eb-f565-53faf54da691",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.524633569872E12,
     "user_tz": -720.0,
     "elapsed": 1054.0,
     "user": {
      "displayName": "Edison",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "103264836563542052392"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 908])\n",
      "[252, 183, 161, 224, 307, 106, 405, 147, 48, 148, 259, 75, 137, 233, 185, 203, 172, 237, 355, 360, 293, 140, 315, 234, 136, 215, 209, 449, 136, 385, 908, 246, 229, 362, 218, 120, 155, 289, 129, 143, 168, 142, 296, 485, 142, 162, 312, 124, 142, 144, 242, 190, 289, 467, 226, 100, 88, 129, 41, 145, 320, 162, 206, 178, 139, 108, 157, 149, 109, 294, 129, 212, 394, 387, 268, 255, 73, 670, 450, 145, 191, 163, 125, 56, 167, 196, 66, 307, 113, 155, 149, 189, 435, 43, 114, 206, 138, 240, 200, 645]\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "ids_batch, x_batch, lengths_batch, y_batch = train_batches[0]\n",
    "print(x_batch.size())\n",
    "print(lengths_batch)\n",
    "print(y_batch.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxR6muZJD1sG",
    "colab_type": "text"
   },
   "source": [
    "## The Pytorch Model\n",
    "### The LSTM\n",
    "![An unrolled RNN.](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)\n",
    "- The LSTM is a special kind of Recurrent Neural Network that will process sequence data and return a vector for each input in our sequence. In the example, given a sequence of inputs $X=x_1, \\ldots , x_t$, the LSTM will give us a sequence of $t$ vectors also called hidden states $H= h_1, \\ldots, h_t$.\n",
    "- The LSTM is a complex beast, in this turorial we will be skipping details on how exactly it works. For more details, visit http://pytorch.org/docs/master/nn.html#lstm\n",
    "- If we think of our input sequence as our word vectors for a given sentence, we can think of the output as a kind of enriched or contextualized version of the input, which will contain not only information about the word each vector represents, but also about its previous words.\n",
    "- In PyTorch, LSTMs will return both the set of output vectors $H$ but also some additional output that we will not pay attention to.\n",
    "- Because we need a fixed-size vector to classify our sentences, we will have to use some kind of pooling function over our hidden states to achieve this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "hqnxzQFEJ_xW",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "def mean_pooling(batch_hidden_states, batch_lengths):\n",
    "    '''\n",
    "    :param batch_hidden_states: torch.Tensor(batch_size, seq_len, hidden_size)\n",
    "    :param batch_lengths: list(batch_size)\n",
    "    :return:\n",
    "    '''\n",
    "    batch_lengths = torch.FloatTensor(batch_lengths)\n",
    "    batch_lengths = batch_lengths.unsqueeze(1)\n",
    "    batch_lengths = Variable(batch_lengths)\n",
    "    if batch_hidden_states.is_cuda:\n",
    "        batch_lengths = batch_lengths.cuda()\n",
    "\n",
    "    pooled_batch = torch.sum(batch_hidden_states, 1)\n",
    "    pooled_batch = pooled_batch / batch_lengths.expand_as(pooled_batch)\n",
    "\n",
    "    return pooled_batch\n",
    "\n",
    "  \n",
    "def max_pooling(batch_hidden_states):\n",
    "    '''\n",
    "    :param batch_hidden_states: torch.Tensor(batch_size, seq_len, hidden_size)\n",
    "    :return:\n",
    "    '''\n",
    "    # the `torch.max()` function will return both the maximum\n",
    "    # value and the argmax over a given, dimension\n",
    "    # as we do not need the argmas, we discard it\n",
    "    pooled_batch, _ = torch.max(batch_hidden_states, 1)\n",
    "    return pooled_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLc-X3xDG9NW",
    "colab_type": "text"
   },
   "source": [
    "- The next key util functions are related to the fact that we are using batches of sentences to train.\n",
    "- To make the training efficient, Pytorch asks us to sort the examples in our batch by sequence length and build a special object.\n",
    "- We will use the function `pack_padded_sequence()` to build this special `PackedSequence` object given our sorted padded batch and the lengths of each sentence on it\n",
    "- Conversely, we will use the `pad_packed_sequence()` function to turn the output of the `nn.LSTM`, a `PackedSequence` object, into a regular Pytorch tensor. This tensor will have zeroes in all padding positions, so we can later directy use our pooling functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "cCyAAJaCG9ey",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "def pack_rnn_input(embedded_sequence_batch, sequence_lengths):\n",
    "    \"\"\"\n",
    "    Prepares the special `PackedSequence` object that can be\n",
    "    efficiently processed by the `nn.LSTM`.\n",
    "    \n",
    "    :param embedded_sequence_batch: torch.Tensor(seq_len, batch_size)\n",
    "    \n",
    "    :param sequence_lengths: list(batch_size)\n",
    "    \n",
    "    :return:\n",
    "      - `PackedSequence` object containing our padded batch\n",
    "      - indices to sort back our sentences to their original order \n",
    "    \"\"\"\n",
    "    sequence_lengths = np.array(sequence_lengths)\n",
    "    sorted_sequence_lengths = np.sort(sequence_lengths)[::-1]\n",
    "\n",
    "    idx_sort = np.argsort(-sequence_lengths)\n",
    "    idx_unsort = np.argsort(idx_sort)\n",
    "\n",
    "    idx_sort = Variable(torch.from_numpy(idx_sort))\n",
    "    idx_unsort = Variable(torch.from_numpy(idx_unsort))\n",
    "\n",
    "    if embedded_sequence_batch.is_cuda:\n",
    "        idx_sort = idx_sort.cuda()\n",
    "        idx_unsort = idx_unsort.cuda()\n",
    "\n",
    "    embedded_sequence_batch = embedded_sequence_batch.index_select(0, idx_sort)\n",
    "\n",
    "    # go back to ints as requested by torch (will change in torch 0.4)\n",
    "    int_sequence_lengths = [int(elem) for elem in sorted_sequence_lengths.tolist()]\n",
    "\n",
    "    # Handling padding in Recurrent Networks\n",
    "    packed_rnn_input = \\\n",
    "        nn.utils.rnn.pack_padded_sequence(embedded_sequence_batch,\n",
    "                                          int_sequence_lengths,\n",
    "                                          batch_first=True)\n",
    "\n",
    "    return packed_rnn_input, idx_unsort\n",
    "\n",
    "  \n",
    "def unpack_rnn_output(packed_rnn_output, indices):\n",
    "    \"\"\"\n",
    "     Recover a regular tensor given a `PackedSequence` as returned\n",
    "     by  `nn.LSTM`\n",
    "\n",
    "    :param packed_rnn_output: torch object\n",
    "    \n",
    "    :param indices: Variable(LongTensor) of indices to sort output\n",
    "    \n",
    "    :return:\n",
    "      - Padded tensor\n",
    "      \n",
    "    \"\"\"\n",
    "    encoded_sequence_batch, _ = \\\n",
    "        nn.utils.rnn.pad_packed_sequence(packed_rnn_output,\n",
    "                                         batch_first=True)\n",
    "\n",
    "    encoded_sequence_batch = \\\n",
    "        encoded_sequence_batch.index_select(0, indices)\n",
    "\n",
    "    return encoded_sequence_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e70TNEbkANUI",
    "colab_type": "text"
   },
   "source": [
    "- To build the model, we extend the `nn.Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "6Bf9pMNtdHWo",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 embeddings,\n",
    "                 hidden_size,\n",
    "                 num_labels,\n",
    "                 input_dropout=0,\n",
    "                 output_dropout=0,\n",
    "                 bidirectional=True,\n",
    "                 num_layers=2,\n",
    "                 pooling='mean'):\n",
    "\n",
    "        super(BiLSTM, self).__init__()\n",
    "\n",
    "        self.embeddings = embeddings\n",
    "        self.pooling = pooling\n",
    "\n",
    "        self.input_dropout = nn.Dropout(input_dropout)\n",
    "        self.output_dropout = nn.Dropout(output_dropout)\n",
    "\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.input_size = self.embeddings.embedding_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(self.input_size,\n",
    "                            hidden_size,\n",
    "                            bidirectional=bidirectional,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True)\n",
    "\n",
    "        self.total_hidden_size = \\\n",
    "            self.hidden_size * 2 if self.bidirectional else self.hidden_size\n",
    "\n",
    "        self.encoder_zero_total_hidden = \\\n",
    "            self.num_layers*2 if self.bidirectional else self.num_layers\n",
    "\n",
    "        self.output_layer = nn.Linear(self.total_hidden_size, self.num_labels)\n",
    "\n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.is_cuda = False\n",
    "\n",
    "    def cuda(self, *args, **kwargs):\n",
    "        super(BiLSTM, self).cuda(*args, **kwargs)\n",
    "        self.is_cuda = True\n",
    "\n",
    "    def cpu(self):\n",
    "        super(BiLSTM, self).cpu()\n",
    "        self.is_cuda = False\n",
    "\n",
    "\n",
    "    def forward(self, sequence_batch, sequence_lengths,\n",
    "                targets=None, train_embeddings=False):\n",
    "\n",
    "        batch_size, seq_len = sequence_batch.size()\n",
    "\n",
    "        embedded_sequence_batch = self.embeddings(sequence_batch)\n",
    "        embedded_sequence_batch = self.input_dropout(embedded_sequence_batch )\n",
    "\n",
    "        packed_rnn_input, indices = pack_rnn_input(embedded_sequence_batch,\n",
    "                                                   sequence_lengths)\n",
    "\n",
    "        rnn_packed_output, _ = self.lstm(packed_rnn_input)\n",
    "        encoded_sequence_batch = unpack_rnn_output(rnn_packed_output, indices)\n",
    "\n",
    "      \n",
    "        if self.pooling == \"mean\":\n",
    "            # batch_size, hidden_x_dirs\n",
    "            pooled_batch = mean_pooling(encoded_sequence_batch,\n",
    "                                        sequence_lengths)\n",
    "\n",
    "        elif self.pooling == \"max\":\n",
    "            # batch_size, hidden_x_dirs\n",
    "            pooled_batch = max_pooling(encoded_sequence_batch)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "       \n",
    "        logits = self.output_layer(pooled_batch)\n",
    "        _, predictions = logits.max(1)\n",
    "\n",
    "        if targets is not None:\n",
    "            loss = self.loss_function(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return loss, predictions, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ipl1kOiKEkac",
    "colab_type": "text"
   },
   "source": [
    "### Instancing our model\n",
    "- Let's define the hyperparameters of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "0qoZmYg689qG",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "hidden_size = 300\n",
    "log_interval = 10\n",
    "num_labels = 2\n",
    "input_dropout = 0.5\n",
    "output_dropout = 0.5\n",
    "bidirectional = True\n",
    "num_layers = 2\n",
    "pooling = 'mean'\n",
    "lr = 0.001\n",
    "gradient_clipping = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "4x850E1O9kM2",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 198.0
    },
    "outputId": "18a1eae5-7dd9-4357-f73f-33233454d7e9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.524633649418E12,
     "user_tz": -720.0,
     "elapsed": 1926.0,
     "user": {
      "displayName": "Edison",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "103264836563542052392"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM(\n",
      "  (embeddings): Embedding(69448, 300, padding_idx=69447)\n",
      "  (input_dropout): Dropout(p=0.5)\n",
      "  (output_dropout): Dropout(p=0.5)\n",
      "  (lstm): LSTM(300, 300, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (output_layer): Linear(in_features=600, out_features=2)\n",
      "  (loss_function): CrossEntropyLoss(\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTM(embeddings=embedddings,\n",
    "               hidden_size=hidden_size,\n",
    "               num_labels=num_labels,\n",
    "               input_dropout=input_dropout,\n",
    "               output_dropout=output_dropout,\n",
    "               bidirectional=bidirectional,\n",
    "               num_layers=num_layers,\n",
    "               pooling=pooling)\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "rBnvsLNC91Ak",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "yaJjF4H2908k",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 5015.0
    },
    "outputId": "7e415584-b3f8-41e8-a293-4fdd89097f28",
    "executionInfo": {
     "status": "error",
     "timestamp": 1.524635674456E12,
     "user_tz": -720.0,
     "elapsed": 2014202.0,
     "user": {
      "displayName": "Edison",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "103264836563542052392"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/10 [00:16<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.763588809967041\n",
      "Train Accuracy: 52.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/10 [00:32<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6884946644306182\n",
      "Train Accuracy: 51.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/10 [00:49<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6568180024623871\n",
      "Train Accuracy: 60.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/10 [01:06<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6618802487850189\n",
      "Train Accuracy: 58.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/10 [01:24<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6577332019805908\n",
      "Train Accuracy: 60.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/10 [01:40<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6528779685497283\n",
      "Train Accuracy: 58.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/10 [01:57<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6298307538032532\n",
      "Train Accuracy: 64.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/10 [02:13<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5931613981723786\n",
      "Train Accuracy: 66.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/10 [02:30<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5972204864025116\n",
      "Train Accuracy: 68.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/10 [02:48<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5573752522468567\n",
      "Train Accuracy: 70.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/10 [03:04<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5268430650234223\n",
      "Train Accuracy: 73.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/10 [03:20<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5649274021387101\n",
      "Train Accuracy: 70.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/10 [03:37<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5199377506971359\n",
      "Train Accuracy: 74.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/10 [03:52<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5426427304744721\n",
      "Train Accuracy: 71.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/10 [04:10<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5652843922376632\n",
      "Train Accuracy: 72.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/10 [04:27<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5644978195428848\n",
      "Train Accuracy: 70.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/10 [04:43<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5058971494436264\n",
      "Train Accuracy: 74.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/10 [04:59<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5395213961601257\n",
      "Train Accuracy: 72.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/10 [05:15<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.49744054973125457\n",
      "Train Accuracy: 74.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/10 [05:32<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4742052495479584\n",
      "Train Accuracy: 76.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/10 [05:47<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4276879608631134\n",
      "Train Accuracy: 79.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/10 [06:04<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.46528694331645964\n",
      "Train Accuracy: 77.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/10 [06:20<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4455619305372238\n",
      "Train Accuracy: 78.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/10 [06:37<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.47218385338783264\n",
      "Train Accuracy: 77.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  10%|█         | 1/10 [08:41<1:18:12, 521.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------\n",
      "Test Loss: 0.48285325622558595\n",
      "Test Accuracy: 75.932\n",
      "---------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  10%|█         | 1/10 [08:57<1:20:40, 537.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.48773019313812255\n",
      "Train Accuracy: 77.81818181818181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  10%|█         | 1/10 [09:14<1:23:06, 554.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4085088938474655\n",
      "Train Accuracy: 80.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  10%|█         | 1/10 [09:30<1:25:34, 570.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3979483425617218\n",
      "Train Accuracy: 82.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  10%|█         | 1/10 [09:48<1:28:12, 588.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.366290745139122\n",
      "Train Accuracy: 83.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  10%|█         | 1/10 [10:05<1:30:50, 605.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3607216000556946\n",
      "Train Accuracy: 84.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  10%|█         | 1/10 [10:22<1:33:19, 622.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3788736552000046\n",
      "Train Accuracy: 83.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  10%|█         | 1/10 [10:38<1:35:50, 638.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3451613008975983\n",
      "Train Accuracy: 85.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  10%|█         | 1/10 [10:55<1:38:17, 655.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.34347525238990784\n",
      "Train Accuracy: 86.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  10%|█         | 1/10 [11:11<1:40:41, 671.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.38172011375427245\n",
      "Train Accuracy: 83.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  10%|█         | 1/10 [11:29<1:43:28, 689.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3215642601251602\n",
      "Train Accuracy: 86.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  10%|█         | 1/10 [11:45<1:45:48, 705.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.292501500248909\n",
      "Train Accuracy: 88.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  10%|█         | 1/10 [12:01<1:48:14, 721.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3556629687547684\n",
      "Train Accuracy: 86.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  10%|█         | 1/10 [12:18<1:50:42, 738.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.34035356491804125\n",
      "Train Accuracy: 85.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  10%|█         | 1/10 [12:33<1:53:02, 753.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3744565635919571\n",
      "Train Accuracy: 83.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  10%|█         | 1/10 [12:51<1:55:45, 771.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.40091287791728974\n",
      "Train Accuracy: 81.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  10%|█         | 1/10 [13:08<1:58:12, 788.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.424839922785759\n",
      "Train Accuracy: 81.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  10%|█         | 1/10 [13:23<2:00:35, 803.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3881752222776413\n",
      "Train Accuracy: 83.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  10%|█         | 1/10 [13:40<2:03:03, 820.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3369844764471054\n",
      "Train Accuracy: 84.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  10%|█         | 1/10 [13:56<2:05:28, 836.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.36284053772687913\n",
      "Train Accuracy: 82.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  10%|█         | 1/10 [14:12<2:07:54, 852.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.31973257213830947\n",
      "Train Accuracy: 85.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  10%|█         | 1/10 [14:27<2:10:09, 867.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2848996058106422\n",
      "Train Accuracy: 88.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  10%|█         | 1/10 [14:44<2:12:44, 884.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.33506195098161695\n",
      "Train Accuracy: 86.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  10%|█         | 1/10 [15:00<2:15:07, 900.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.32450890690088274\n",
      "Train Accuracy: 87.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  10%|█         | 1/10 [15:18<2:17:47, 918.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2946134254336357\n",
      "Train Accuracy: 88.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  20%|██        | 2/10 [17:22<1:09:29, 521.20s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------\n",
      "Test Loss: 0.3349680826663971\n",
      "Test Accuracy: 86.096\n",
      "---------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  20%|██        | 2/10 [17:38<1:10:35, 529.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3343262568116188\n",
      "Train Accuracy: 87.9090909090909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  20%|██        | 2/10 [17:55<1:11:40, 537.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.26948550939559934\n",
      "Train Accuracy: 89.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  20%|██        | 2/10 [18:11<1:12:46, 545.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2691680908203125\n",
      "Train Accuracy: 88.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  20%|██        | 2/10 [18:29<1:13:56, 554.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.25346323400735854\n",
      "Train Accuracy: 90.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  20%|██        | 2/10 [18:46<1:15:06, 563.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2357587218284607\n",
      "Train Accuracy: 91.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  20%|██        | 2/10 [19:03<1:16:12, 571.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2458162397146225\n",
      "Train Accuracy: 90.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  20%|██        | 2/10 [19:20<1:17:20, 580.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.21496859788894654\n",
      "Train Accuracy: 91.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  20%|██        | 2/10 [19:36<1:18:25, 588.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2256816953420639\n",
      "Train Accuracy: 91.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  20%|██        | 2/10 [19:52<1:19:29, 596.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2530245453119278\n",
      "Train Accuracy: 90.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  20%|██        | 2/10 [20:10<1:20:43, 605.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.21294230073690415\n",
      "Train Accuracy: 92.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  20%|██        | 2/10 [20:26<1:21:46, 613.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.18570363894104958\n",
      "Train Accuracy: 92.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  20%|██        | 2/10 [20:42<1:22:51, 621.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2633150011301041\n",
      "Train Accuracy: 89.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  20%|██        | 2/10 [20:59<1:23:57, 629.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.23298424705863\n",
      "Train Accuracy: 90.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  20%|██        | 2/10 [21:15<1:25:00, 637.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.26107673943042753\n",
      "Train Accuracy: 88.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  20%|██        | 2/10 [21:33<1:26:12, 646.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.28451064229011536\n",
      "Train Accuracy: 88.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  20%|██        | 2/10 [21:49<1:27:17, 654.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2503566339612007\n",
      "Train Accuracy: 89.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  20%|██        | 2/10 [22:05<1:28:21, 662.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2561042934656143\n",
      "Train Accuracy: 89.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  20%|██        | 2/10 [22:21<1:29:27, 670.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.23490295112133025\n",
      "Train Accuracy: 90.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  20%|██        | 2/10 [22:38<1:30:32, 679.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2658605217933655\n",
      "Train Accuracy: 89.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  20%|██        | 2/10 [22:54<1:31:37, 687.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.21524795591831208\n",
      "Train Accuracy: 91.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  20%|██        | 2/10 [23:09<1:32:37, 694.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2024135947227478\n",
      "Train Accuracy: 92.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  20%|██        | 2/10 [23:26<1:33:45, 703.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.21830800622701646\n",
      "Train Accuracy: 91.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  20%|██        | 2/10 [23:42<1:34:49, 711.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.20518997088074684\n",
      "Train Accuracy: 91.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  20%|██        | 2/10 [24:00<1:36:00, 720.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2105241060256958\n",
      "Train Accuracy: 91.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  30%|███       | 3/10 [26:04<1:00:49, 521.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------\n",
      "Test Loss: 0.4250059585571289\n",
      "Test Accuracy: 82.0\n",
      "---------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  30%|███       | 3/10 [26:20<1:01:28, 526.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.31855655908584596\n",
      "Train Accuracy: 87.54545454545455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  30%|███       | 3/10 [26:36<1:02:05, 532.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2498329296708107\n",
      "Train Accuracy: 89.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  30%|███       | 3/10 [26:53<1:02:44, 537.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.21977973729372025\n",
      "Train Accuracy: 91.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  30%|███       | 3/10 [27:10<1:03:25, 543.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1705862559378147\n",
      "Train Accuracy: 93.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  30%|███       | 3/10 [27:28<1:04:06, 549.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.18244198113679885\n",
      "Train Accuracy: 94.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  30%|███       | 3/10 [27:45<1:04:45, 555.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.14158847630023957\n",
      "Train Accuracy: 94.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  30%|███       | 3/10 [28:01<1:05:24, 560.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.14569939598441123\n",
      "Train Accuracy: 94.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  30%|███       | 3/10 [28:18<1:06:02, 566.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.18467241451144217\n",
      "Train Accuracy: 92.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  30%|███       | 3/10 [28:34<1:06:39, 571.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.16815004944801332\n",
      "Train Accuracy: 93.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  30%|███       | 3/10 [28:52<1:07:23, 577.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.15554785653948783\n",
      "Train Accuracy: 94.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  30%|███       | 3/10 [29:08<1:08:00, 582.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.19332499839365483\n",
      "Train Accuracy: 92.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  30%|███       | 3/10 [29:24<1:08:37, 588.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.23857715874910354\n",
      "Train Accuracy: 90.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  30%|███       | 3/10 [29:41<1:09:16, 593.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.171990504860878\n",
      "Train Accuracy: 93.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  30%|███       | 3/10 [29:56<1:09:52, 598.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.17010625824332237\n",
      "Train Accuracy: 93.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  30%|███       | 3/10 [30:14<1:10:34, 604.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.16212233826518058\n",
      "Train Accuracy: 93.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  30%|███       | 3/10 [30:31<1:11:12, 610.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.21549870669841767\n",
      "Train Accuracy: 92.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  30%|███       | 3/10 [30:47<1:11:50, 615.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2057577170431614\n",
      "Train Accuracy: 92.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  30%|███       | 3/10 [31:03<1:12:28, 621.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.15492613911628722\n",
      "Train Accuracy: 95.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  30%|███       | 3/10 [31:19<1:13:06, 626.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1912270851433277\n",
      "Train Accuracy: 92.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  30%|███       | 3/10 [31:36<1:13:44, 632.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.16652667969465257\n",
      "Train Accuracy: 93.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  30%|███       | 3/10 [31:51<1:14:19, 637.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1617109827697277\n",
      "Train Accuracy: 94.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  30%|███       | 3/10 [32:08<1:14:59, 642.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.15191135480999945\n",
      "Train Accuracy: 94.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  30%|███       | 3/10 [32:24<1:15:36, 648.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2094196744263172\n",
      "Train Accuracy: 92.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  30%|███       | 3/10 [32:41<1:16:17, 653.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.19741350561380386\n",
      "Train Accuracy: 92.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-4a39df67a200>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m         loss, predictions, logits = model.forward(padded_x_slice,\n\u001b[1;32m     40\u001b[0m                                                   \u001b[0mx_slice_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                                                   y_slice)\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_slice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-5ccf6b79422d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sequence_batch, sequence_lengths, targets, train_embeddings)\u001b[0m\n\u001b[1;32m     63\u001b[0m                                                    sequence_lengths)\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mrnn_packed_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_rnn_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mencoded_sequence_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpack_rnn_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_packed_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         )\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhack_onnx_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36m_do_forward\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0mflat_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_iter_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0mflat_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNestedIOFunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mflat_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0mnested_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0mnested_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mnested_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_map_variable_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_extended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward_extended\u001b[0;34m(self, input, weight, hx)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mhy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/backends/cudnn/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(fn, input, hx, weight, output, hy)\u001b[0m\n\u001b[1;32m    303\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcy_desc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                 \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m                 \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreserve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreserve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m             ))\n\u001b[1;32m    307\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "pbar = tqdm.trange(epochs, desc='Training...')\n",
    "\n",
    "for epoch in pbar:\n",
    "    epoch_correct = 0\n",
    "    epoch_total = 0\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(train_batches):\n",
    "        (id_sice, padded_x_slice, x_slice_lengths, y_slice) = batch\n",
    "        loss, predictions, logits = model.forward(padded_x_slice,\n",
    "                                                  x_slice_lengths,\n",
    "                                                  y_slice)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), gradient_clipping)\n",
    "\n",
    "        optimizer.step()\n",
    "        correct = (predictions == y_slice).long().sum()\n",
    "        total = y_slice.size(0)\n",
    "        epoch_correct += correct.data[0]\n",
    "        epoch_total += total\n",
    "        epoch_loss += loss.data[0]\n",
    "\n",
    "        if i % log_interval == 0 and i > 0:\n",
    "            accuracy = 100 * epoch_correct/ epoch_total\n",
    "\n",
    "            pbar.write('Train Loss: {}'.format(epoch_loss/log_interval))\n",
    "            pbar.write('Train Accuracy: {}'.format(accuracy))\n",
    "            epoch_correct = 0\n",
    "            epoch_total = 0\n",
    "            epoch_loss = 0\n",
    "\n",
    "    test_epoch_correct = 0\n",
    "    test_epoch_total = 0\n",
    "    test_epoch_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(test_batches):\n",
    "        (id_sice, padded_x_slice, x_slice_lengths, y_slice) = batch\n",
    "        loss, predictions, logits = model.forward(padded_x_slice,\n",
    "                                                  x_slice_lengths,\n",
    "                                                  y_slice)\n",
    "\n",
    "        correct = (predictions == y_slice).long().sum()\n",
    "        total = y_slice.size(0)\n",
    "        test_epoch_correct += correct.data[0]\n",
    "        test_epoch_total += total\n",
    "        test_epoch_loss += loss.data[0]\n",
    "\n",
    "    test_accuracy = 100 * test_epoch_correct / test_epoch_total\n",
    "\n",
    "    pbar.write('\\n---------------------')\n",
    "    pbar.write('Test Loss: {}'.format(test_epoch_loss/len(test_batches)))\n",
    "    pbar.write('Test Accuracy: {}'.format(test_accuracy))\n",
    "    pbar.write('---------------------\\n')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "pytorch_sequence_classification.ipynb",
   "version": "0.3.2",
   "views": {},
   "default_view": {},
   "provenance": [
    {
     "file_id": "10ROgEz2MhfNJLQvHdqiaKxv-f5QGF1BE",
     "timestamp": 1.523788164178E12
    }
   ],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}